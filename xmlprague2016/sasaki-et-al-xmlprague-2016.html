<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>From XML to RDF step by step: Approaches for Leveraging XML Workflows with Linked Data</title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="article"><div class="titlepage"><div><div><h1 class="title"><a name="d5e1"></a>From XML to RDF step by step: Approaches for Leveraging XML Workflows with Linked Data</h1></div><div><div class="author"><h3 class="author"><span class="firstname">Marta</span> <span class="surname">Borriello</span></h3><div class="affiliation"><span class="orgname">Vistatec<br></span></div><code class="email">&lt;<a class="email" href="mailto:marta.borriello@vistatec.com">marta.borriello@vistatec.com</a>&gt;</code></div></div><div><div class="author"><h3 class="author"><span class="firstname">Christian</span> <span class="surname">Dirschl</span></h3><div class="affiliation"><span class="orgname">Wolters Kluwer Germany<br></span></div><code class="email">&lt;<a class="email" href="mailto:cdirschl@wolterskluwer.de">cdirschl@wolterskluwer.de</a>&gt;</code></div></div><div><div class="author"><h3 class="author"><span class="firstname">Axel</span> <span class="surname">Polleres</span></h3><code class="email">&lt;<a class="email" href="mailto:axel.polleres@wu.ac.at">axel.polleres@wu.ac.at</a>&gt;</code></div></div><div><div class="author"><h3 class="author"><span class="firstname">Phil</span> <span class="surname">Ritchie</span></h3><code class="email">&lt;<a class="email" href="mailto:philr@vistatec.ie">philr@vistatec.ie</a>&gt;</code></div></div><div><div class="author"><h3 class="author"><span class="firstname">Frank</span> <span class="surname">Salliau</span></h3><code class="email">&lt;<a class="email" href="mailto:frank.salliau@ugent.be">frank.salliau@ugent.be</a>&gt;</code></div></div><div><div class="author"><h3 class="author"><span class="firstname">Felix</span> <span class="surname">Sasaki</span></h3><code class="email">&lt;<a class="email" href="mailto:felix.sasaki@dfki.de">felix.sasaki@dfki.de</a>&gt;</code></div></div><div><div class="author"><h3 class="author"><span class="firstname">Giannis</span> <span class="surname">Stoitsis</span></h3><code class="email">&lt;<a class="email" href="mailto:stoitsis@agroknow.com">stoitsis@agroknow.com</a>&gt;</code></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl class="toc"><dt><span class="sect1"><a href="#s1">Introduction</a></span></dt><dd><dl><dt><span class="sect2"><a href="#s1-1">Motivation</a></span></dt><dt><span class="sect2"><a href="#s1-2">The Relation to RDF Chimera</a></span></dt><dt><span class="sect2"><a href="#s1-3">Background: The FREME Project</a></span></dt></dl></dd><dt><span class="sect1"><a href="#s2">Business Case Motivation Examples</a></span></dt><dd><dl><dt><span class="sect2"><a href="#s2-1">The Case of Agro-Know and Wolters Kluwer - Linked Data in XML Publishing Workflows</a></span></dt><dt><span class="sect2"><a href="#s2-2">The Case of Vistatec - Linked Data in XML Localization Workflows</a></span></dt><dt><span class="sect2"><a href="#s2-3">The Case of iMinds - Linked Data in Book Metadata</a></span></dt></dl></dd><dt><span class="sect1"><a href="#s3">Approaches for Linked Data Integration in XML Workflows</a></span></dt><dd><dl><dt><span class="sect2"><a href="#s3-1">Approach 1: Convert XML to Linked Data</a></span></dt><dt><span class="sect2"><a href="#s3-2">Approach 2: Embedd Linked Data into XML via Structured Markup</a></span></dt><dt><span class="sect2"><a href="#s3-3">Approach 3: Anchor Linked Data in XML Attributes</a></span></dt><dt><span class="sect2"><a href="#s3-4">Approach 4: Embed Linked Data in Metadata Sections of XML Files</a></span></dt><dt><span class="sect2"><a href="#s3-5">Approach 5: Anchor Linked Data via Annotations in XML Content</a></span></dt></dl></dd><dt><span class="sect1"><a href="#s4">Relating Business Cases and Integration Approaches</a></span></dt><dt><span class="sect1"><a href="#s5">Routes to bridge between RDF and XML</a></span></dt><dt><span class="sect1"><a href="#s-6">Conclusion</a></span></dt><dt><span class="sect1"><a href="#s-ack">Acknowledgments</a></span></dt><dt><span class="bibliography"><a href="#sec-references">Bibliography</a></span></dt></dl></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s1"></a>Introduction</h2></div></div></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s1-1"></a>Motivation</h3></div></div></div><p>There have been many discussions about benefits and drawbacks of XML vs. RDF. In practice more and more XML and linked data technologies are being used together. This leads to opportunities and uncertainties: for years companies have invested heavily in XML workflows. They are not willing to throw them away for the benefits of linked data.</p><p>In XML workflows XML content is</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Generated, from scratch or based on existing content; </p></li><li class="listitem"><p>processed, e.g.: validated, queried, transformed; and</p></li><li class="listitem"><p>stored in various forms, e.g.: as XML, in a different format (e.g. PDF / HTML output); and</p></li><li class="listitem"><p>potentially input to other XML or non-XML workflows.</p></li></ul></div><p>Each part of the workflow may include huge amounts of XML data. This can be XML files themselves, but also additional related items like: XSLT or XSL-FO stylesheets for transformation or printing, XQuery based queries, or XML Schema / DTD / Relax NG schemata for validation etc.</p><p>For many potential users of linked data, giving up these workflows is not an option. Also, changing even a small part of the workflow may lead to high costs. Imagine one element <code class="code">linkedDataStorage</code> added to an imaginary XML document:</p><div class="example"><a name="ex1"></a><p class="title"><b>Example&nbsp;1.&nbsp;XML document with linked data embedded in an element</b></p><div class="example-contents"><pre class="programlisting">
&lt;myData&gt;
 &lt;head&gt;...&lt;/head&gt;
 &lt;body&gt;
 <span class="bold"><strong>&lt;linkedDataStorage&gt;...&lt;/linkedDataStorage&gt;</strong></span> ...
 &lt;/body&gt;
&lt;/myData&gt;
</pre></div></div><br class="example-break"><p>Adding this element to an XML element may break various aspects of the workflow,
            like:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Validation: the underlying schema does not understand the new element.</p></li><li class="listitem"><p> Transformation: a transformation may expect a certain element as the first
                        child element of body. If <code class="code">linkedDataStorage</code> is the first child,
                        the transformation would fail.</p></li></ul></div><p>One may argue that good XML schema will leave space for expansion using lax validated parts or by accepting attributes from other namespaces, for example. Nevertheless, in practice we have to work with a lot of existing XML Schemas, related tooling and workflows. So creating extension points and deploying lax validation may not be an option in real life.</p><p>Whereas the strict validation against schemas on the one hand is in the XML world
            often seen as a feature of the toolchain, on the other hand, such adding of elements and
            schemaless integration of different (parts of) datasets is actually one of the main
            &#8220;selling points of RDF&#8221;. However, note that on the contrary, even in the RDF world,
            users are starting to demand tools for stricter schema validation, which has recently
            lead to the foundation of a respective working group around RDF Data Shapes in
            W3C.<a href="#ftn.d5e114" class="footnote" name="d5e114"><sup class="footnote">[1]</sup></a> So, overall there seems to be lots to learn for
            both sides from each other.</p><p>This paper wants to help with XML and RDF integration to foster incremental adoption
                of linked data, without the need to get rid of existing XML workflows. We are
                discussing  various integration approaches. They all have benefits and drawbacks.
                The reader needs to be careful in deciding which approach to choose.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s1-2"></a>The Relation to RDF Chimera</h3></div></div></div><p>In her keynote at XML Prague 2012 and a subsequent blog post, Jeni Tennison
                discussed <a class="link" href="http://www.jenitennison.com/2012/06/30/rdf-chimera.html" target="_top">RDF chimera</a>. She is arguing that for representing RDF, syntaxes like
                RDF/XML or JSON or JSON-LD should be seen as a means to achieve something - a road,
                but not a destination. An example is a query to an RDF data store, and the outcome
                is represented in an HTML page.</p><p> The goal of our paper is different. We assume that there is existing content that
                benefits from <span class="italic">data integration</span> with linked data -
                without turning the content into a linked data model. Let&#8217;s look at an example:
                imagine we have the sentence <code class="code">Berlin is the capital of Germany!</code>. There
                are many linked data sources like <a class="link" href="http://dbpedia.org/about" target="_top">DBpedia</a> that contain information about <code class="code">Berlin</code>; it would add
                an enormous value to existing content (or content creation workflows) if such
                information could be taken into account. This does not mean - like in the case of
                RDF chimera - to give up the XML based workflows, but to provide means for the data
                integration. In this view we can see the linked data process as a type of
                enrichment, hence we call the process <span class="italic">enriching XML
                    content</span> with linked data based information.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s1-3"></a>Background: The FREME Project</h3></div></div></div><p>FREME<a href="#ftn.d5e131" class="footnote" name="d5e131"><sup class="footnote">[2]</sup></a> is an European project funded under the H2020
                Framework Programme. FREME is providing a set of
                interfaces (APIs and GUIs) for multilingual and semantic enrichment of digital
                content. The project started in February 2015, will last for two years and
                encompasses eight partners. The partners provide technology from the realm of
                language and data processing, business cases from various domains, and expertise in
                business modeling. This expertise is of specific importance since both language and
                linked data technologies are not yet widely adopted. The challenge of XML
                re-engineering for the sake of linked data processing is one hindrance that needs to
                be overcome to achieve more adoption.</p><p>FREME provides six e-Services for processing of digital content:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>e-Internationalisation based on the Internationalisation Tag Set (ITS)
                        Version 2.0.</p></li><li class="listitem"><p>e-Link based on the Natural Language Processing Interchange Format (NIF)
                        and linked (open) data sources.</p></li><li class="listitem"><p>e-Entity based on entity recognition software and existing linked entity
                        datasets.</p></li><li class="listitem"><p>e-Terminology based on cloud terminology services for terminology
                        management and terminology annotation web service to annotate terminology in
                        ITS 2.0 enriched content.</p></li><li class="listitem"><p>e-Translation based on cloud machine translation services for building
                        custom machine translation systems.</p></li><li class="listitem"><p>e-Publishing based on cloud content authoring environment (for example
                        e-books, technical documentation, marketing materials etc.) and its export
                        for publishing in Electronic Publication (EPUB3) format.</p></li></ul></div><p>This paper will not provide details about the services - examples and more
                information on FREME can be found at <a class="link" href="http://api.freme-project.eu/doc/current/" target="_top">http://api.freme-project.eu/doc/current/</a></p><p> All e-services have in common that XML content is a potential input and output
                format: via FREME, XML content can be enriched with additional information, to add
                value to the content. But FREME is only one example: many other linked data projects
                involve companies working with linked data content. </p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s2"></a>Business Case Motivation Examples</h2></div></div></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s2-1"></a>The Case of Agro-Know and Wolters Kluwer - Linked Data in XML Publishing Workflows</h3></div></div></div><p>Agro-Know is data oriented company that helps organisations to manage, organise
                and open their agricultural and food information. One of the main activities of
                Agro-Know is the aggregation of bibliographic references from diverse sources to
                support online search services like <a class="link" href="http://agris.fao.org/" target="_top">AGRIS</a> of the Food and Agricultural Organisation of the United Nations.
                Agro-Know is doing so by aggregating metadata records from data providers such as
                journals, small publishers, universities, research centers, libraries and national
                aggregators. The metadata aggregation workflow of Agro-Know includes parts for
                metadata analysis, harvesting, filtering, transformation, enrichment, indexing and
                publishing. The main goal of applying the different steps of the workflow is to end
                up with a well formated and complete metadata record that is compatible to the
                metadata standard for agricultural sciences, namely <a class="link" href="http://www.fao.org/docrep/008/ae909e/ae909e00.HTM" target="_top">AGRIS AP</a>.
                The majority of the metadata records that are collected are in XML following several
                metadata formats such as DC, AGRIS AP, DOAJ, MODS, MARC 21 etc. The processed
                metadata records are published in AGRIS AP, JSON and RDF.</p><p>Within such metadata aggregation workflow, Agro-Know is facing several challenges
                related to the enrichment of the metadata records. One such example is
                non-structured information about authors that in many cases is not following an
                authority file and includes additional information in the same XML tag like
                affiliation, email and location. This information cannot be automatically
                transformed to structured information and remaining in the tag, it reduces the
                quality of provided filtering options in the search functionality of the online
                service. In addition to that, since an authority file is not used on the data
                provider side, this results in an ambiguity problem as the same author may appear
                with many different variations of the name. Another problem is the absence of
                subject terms in the metadata records from a multilingual vocabulary such as <a class="link" href="http://aims.fao.org/vest-registry/vocabularies/agrovoc-multilingual-agricultural-thesaurus" target="_top">AGROVOC</a>, that consists of more than 32.000 terms available in 23
                languages. Including AGROVOC terms in the metadata records can semantically enhance
                the information and can enable better discovering services at the front end
                application. </p><p>To solve such problems, Agro-Know is using the FREME e-services in order to
                improve a) the online services that are offered to the end users and b) the
                semantics of the metadata records that is provided to other stakeholders of this
                data value chain, such as publishers. The main goal will be to add the structured
                information in the XML records by keeping the level of intervention at a minimum
                level in order to eliminate the revisions required in the existing workflows.
                Examples of how an XML part referring to authors can be enriched using FREME
                e-services is presented in the table below. In this case, including the <a class="link" href="http://orcid.org/" target="_top">ORCID</a> identifier may help in
                disambiguation but also in the enrichment of the information as we can show to the
                end user of the online service additional valuable information directly retrieved
                from ORCID. </p><div class="informaltable"><table border="1"><colgroup><col align="left" class="col1"><col align="left" class="col2"></colgroup><tbody><tr><td align="left"><span class="bold"><strong>Before FREME</strong></span></td><td align="left"><span class="bold"><strong>Result of deploying
                                FREME</strong></span></td></tr><tr><td align="left">
                                <pre class="programlisting">&lt;dc:creator&gt;
&lt;ags:creatorPersonal&gt;
Stoitsis, Giannis,
Agroknow
&lt;/ags:creatorPersonal&gt;
&lt;/dc:creator&gt;</pre>
                            </td><td align="left">
                                <pre class="programlisting">&lt;dc:creator&gt;
&lt;ags:creatorPersonal&gt;Stoitsis,
Giannis&lt;/ags:creatorPersonal&gt;
<span class="bold"><strong>&lt;nameIdentifier schemeURI=
"http://orcid.org/"
 nameIdentifierScheme=
"ORCID"&gt;0000-0003-3347-8265
&lt;/nameIdentifier&gt;
&lt;affiliation&gt;Agroknow&lt;/affiliation&gt;</strong></span>
&lt;/dc:creator&gt;</pre>
                            </td></tr><tr><td align="left">
                                <pre class="programlisting">&lt;dc:subject&gt;
&lt;ags:subjectClassification
scheme="ags:ASC"&gt;
&lt;![CDATA[J10]]&gt;
&lt;/ags:subjectClassification&gt;
&lt;/dc:subject&gt;</pre>
                            </td><td align="left">
                                <pre class="programlisting">&lt;dc:subject
<span class="bold"><strong>freme-enrichment=
"http://aims.fao.org/aos/agrovoc/c_426
 http://aims.fao.org/aos/agrovoc/c_24135
 http://aims.fao.org/aos/agrovoc/c_4644
 http://aims.fao.org/aos/agrovoc/c_7178"&gt;</strong></span>
&lt;ags:subjectClassification scheme=
 "ags:ASC"&gt;&lt;![CDATA[J10]]&gt;
&lt;/ags:subjectClassification&gt;
&lt;/dc:subject&gt;</pre>
                            </td></tr></tbody></table></div><p>Wolters Kluwer is a global information service provider with businesses mainly in
                the legal, tax, health and financial market. The fundamental transformation process
                from folio to digital and service offerings that is currently disrupting the whole
                industry requires also more efficient and streamlined production processes. In the
                last ten years, the industry has very much focused on XML based publishing
                workflows, where all relevant information resides as metadata within the documents,
                which are structured according to proprietary DTDs or XML schemas. The industry is
                slowly starting to adapt linked data principles, because they offer the required
                flexibility, scalability and information interoperability that XML or also
                relational database models do not offer. As a first step, metadata is extracted from
                the documents and stored in parallel in graph databases. Already this step requires
                a major shift in technology as well as business culture, because the focus and
                added-value moves away from pure content to data and information.</p><p>Wolters Kluwer Health is customer of Agro-know and has integrated its XML delivery
                channel for enriched scientific references mainly in the area of agriculture.
                Agro-know is offering more and more added value services using linked data
                principles and in this course reduces the traditional XML-based delivery pipeline
                step by step in order to stimulate usage of the superior channels. This change
                causes major challenges at the customer&#8217;s side. Semantic Web technology and
                standards are not yet common solutions in publishing. Therefore technical
                infrastructure as well as skills have to be developed in order to get things even
                started. This requires a certain transition period, where the old delivery channel
                remains stable and the customer can prepare the changes.</p><p> In such a scenario, Wolters Kluwer recommends that the source provider enables
                the customer to locally keep his old production workflow in place as long as it is
                needed. This could be achieved e.g. by making the conversion script available as
                open source. In addition, a proper documentation about the differences from old to
                new is also vital for success. Ideally, a direct communication flow between vendor
                and customer would help to lower concerns and accelerate uptake of the new
                process.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s2-2"></a>The Case of Vistatec - Linked Data in XML Localization Workflows</h3></div></div></div><p>Vistatec is a leading provider of digital content translation and locale
                adaptation services for international businesses. These businesses use a variety of
                Vistatec multilingual services to publish a range of content types including:
                corporate communications; marketing collateral; e-commerce catalogues; and product,
                travel destination, and leisure experience descriptions. </p><p>Vistatec's production processes are highly automated and based on XML standards
                for systems integration, process component interoperability and the capture and use
                of metadata.</p><p>The localization process, content types and end consumers of the content all
                benefit greatly from FREME semantic enrichment and entity discovery and linking
                e-services.</p><p>The successful adoption of technology hinges upon the ease of use. Vistatec has
                adapted its open source <a class="link" href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xliff" target="_top">XLIFF</a> editor, <a class="link" href="http://open.vistatec.com/ocelot/index.php?title=Main_Page" target="_top">Ocelot</a>, to consume FREME e-services in a transparent and optimal way
                using configurable pipelines.</p><p> The table below summarizes the steps of a typical localization workflow and the
                benefits that can be realized through the use of FREME e-services:</p><div class="informaltable"><table border="1"><colgroup><col align="left" class="col1"><col align="left" class="col2"><col align="left" class="col3"></colgroup><tbody><tr><td align="left"><span class="bold"><strong>Process Step</strong></span></td><td align="left"><span class="bold"><strong>FREME e-service</strong></span></td><td align="left"><span class="bold"><strong>Benefit</strong></span></td></tr><tr><td align="left">Conversion of native document to Extensible Localization
                                Interchange File Format</td><td align="left">e-Internationalization</td><td align="left">Define translatable portions of the document.</td></tr><tr><td align="left">Translation</td><td align="left">e-Terminology and e-Entity</td><td align="left">These services help linguists to identify and use appropriate
                                translations suitable for the subject domain.</td></tr><tr><td align="left">Semantic enrichment</td><td align="left">e-Link</td><td align="left">Suggest information resources which relate to the subject matter
                                of the content.</td></tr><tr><td align="left">Content publication</td><td align="left">e-Pub</td><td align="left">Incorporation of markup for entity definitions and hyperlinks to
                                information resources which relate closely to the subject matter of
                                the content.</td></tr></tbody></table></div><p>A tutorial from the FREME documentation<a href="#ftn.d5e226" class="footnote" name="d5e226"><sup class="footnote">[3]</sup></a> shows how
                XLIFF can be processed via FREME e-Services. We process the following XLIFF element,
                using the example sentence from a previous section:</p><pre class="programlisting">&lt;source&gt;Berlin is the capital of Germany!&lt;/source&gt;</pre><p>The e-Entity service identifies <code class="code">Berlin</code> as a named entity with a
                certain type and a unique URI. The result of the process is not stored in XML, but
                as a linked data representation including <span class="bold"><strong>offsets</strong></span>
                that point to the string content. The URI </p><pre class="programlisting">&lt;http://freme-project.eu/#char=25,32&gt; </pre><p>identifies the entity, offsets and entity related information. A complete linked
                data representation, using the turtle syntax, looks as follows.</p><div class="example"><a name="ex-2"></a><p class="title"><b>Example&nbsp;2.&nbsp;Linked data representation using offsets for pointing to existing XML content</b></p><div class="example-contents"><pre class="programlisting">&lt;http://freme-project.eu/#char=25,32&gt; ...
(1) nif:anchorOf          "Germany"^^xsd:string ;
(2) nif:beginIndex        "25"^^xsd:int ;
(3) nif:endIndex          "32"^^xsd:int ; ...
(4) itsrdf:taClassRef     &lt;http://nerd.eurecom.fr/ontology#Location&gt;;
(5) itsrdf:taIdentRef     &lt;http://dbpedia.org/resource/Germany&gt;.</pre></div></div><br class="example-break"><p>The linked data statements expressed in above representation are:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The annotation is (1) anchored in the string Germany.</p></li><li class="listitem"><p>The annotation (2) starts at character 25 and (3) ends at character
                        32.</p></li><li class="listitem"><p>The annotation is related to (4) the class URI
                        http://nerd.eurecom.fr/ontology#Location.</p></li><li class="listitem"><p>The entity is (5) uniquely identified with the URI
                        http://dbpedia.org/resource/Germany. </p></li></ul></div><p>The example should make clear why we are not looking at a data conversion task
                (like in the discussion on RDF chimera), but at a data integration task. We don&#8217;t
                aim at changing the XLIFF source element, but at relating it to the information
                provided via the linked data representations. The data integration approaches
                discussed in <a class="xref" href="#s3" title="Approaches for Linked Data Integration in XML Workflows">the section called &#8220;Approaches for Linked Data Integration in XML Workflows&#8221;</a> are ways to achieve this goal.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s2-3"></a>The Case of iMinds - Linked Data in Book Metadata</h3></div></div></div><p>iMinds, Flanders&#8217; digital research hub, conducts research on book publishing in
                close collaboration with the Flemish publishing industry. An important aspect in
                book publishing is book metadata. As the volume in published books increases, and as
                the book industry becomes more and more digital, book metadata also becomes
                increasingly important: book publishers want their books to be found on retail
                sites. Correct and rich metadata is a prerequisite for online
                discoverability.</p><p>Within the publishing value chain, stakeholders have since long agreed to use a
                standard format for book metadata: ONIX for Books<a href="#ftn.d5e254" class="footnote" name="d5e254"><sup class="footnote">[4]</sup></a>.
                This XML-based standard has been adopted worldwide and is used by publishers to
                communicate book product information in a consistent way in electronic form.</p><p>ONIX for Books is developed and maintained by EditEUR<a href="#ftn.d5e258" class="footnote" name="d5e258"><sup class="footnote">[5]</sup></a>, the international group coordinating development of the standards
                infrastructure for electronic commerce in the book, e-book and serials sectors. The
                ONIX for Books standard and corresponding workflow are solidly embedded in the
                publishing retail chain, from publisher to distributor to retailer. Migrating to
                other technologies, e.g. using linked data, requires a substantial investment which
                stakeholders in this industry are very reluctant to make. </p><p>We do not recommend to substitute this standard with a fully fledged linked data
                approach. However, we find there are cases where linked data can be beneficial as
                enrichment of existing ONIX metadata. The example below shows a possible usage of
                linked data in ONIX.</p><p><span class="bold"><strong>Author information as linked data</strong></span>: An ONIX file typically contains information about the author(s) of a book. These
                are called contributors (other types of contributors are illustrators, translators
                etc.).</p><p>Below you can find a block of metadata with information about the author of a
                book, Jonathan Franzen. A possible enrichment with linked data might be to insert
                the link to the authority record about Jonathan Franzen on viaf.org, via the Entity
                tag. Please note that these tags are not valid within the current ONIX schema and
                are used here merely as an example of a possible enrichment.</p><p>This enrichment may prove useful in several ways:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>disambiguation of the author (using the VIAF identifier); and</p></li><li class="listitem"><p>providing a link to more information on the author.</p></li></ul></div><div class="example"><a name="ex-3"></a><p class="title"><b>Example&nbsp;3.&nbsp;A potential approach for embedding linked data identifiers into ONIX</b></p><div class="example-contents"><pre class="programlisting">&lt;Contributor&gt;
 &lt;NameIdentifier&gt;
  &lt;NameIDType&gt;
   &lt;IDTypeName&gt;Meta4Books ContributorID&lt;/IDTypeName&gt;
   &lt;IDValue&gt;65097&lt;/IDValue&gt;
  &lt;/NameIDType&gt;
 &lt;/NameIdentifier&gt;
 &lt;ContributorRole&gt;A01&lt;/ContributorRole&gt;
 &lt;SequenceNumber&gt;1&lt;/SequenceNumber&gt;
 &lt;NamesBeforeKey&gt;Jonathan&lt;/NamesBeforeKey&gt;
 &lt;KeyNames&gt;Franzen&lt;/KeyNames&gt;
 <span class="bold"><strong>&lt;Entity&gt;    
  &lt;URI&gt;http://viaf.org/viaf/84489381/&lt;/URI&gt;
 &lt;/Entity&gt;</strong></span>
&lt;/Contributor&gt;</pre></div></div><br class="example-break"></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s3"></a>Approaches for Linked Data Integration in XML Workflows</h2></div></div></div><p>The following approaches are a non exhaustive list. The aim is to provide examples
            that are currently in use and show their benefits and drawbacks. The examples are
            anchored in the business cases described in <a class="xref" href="#s2" title="Business Case Motivation Examples">the section called &#8220;Business Case Motivation Examples&#8221;</a>. The structure of the approach description is always
            as follows:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Name the example;</p></li><li class="listitem"><p>Explain what actually happens with some XML code snippets; and</p></li><li class="listitem"><p>Explain drawbacks and benefits, both from a linked data and an XML processing
                    point of view.</p></li></ul></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s3-1"></a>Approach 1: Convert XML to Linked Data</h3></div></div></div><p><span class="bold"><strong>What actually happens:</strong></span> XML is converted into
                linked data. The XML content itself is not touched, but an additional set of data,
                i.e. a linked data representation is created.</p><pre class="programlisting">&lt;xs:element name="lingualityType"&gt;
 &lt;xs:simpleType&gt;
  &lt;xs:restriction base="xs:string"&gt;
   &lt;xs:enumeration value="monolingual"/&gt;
   &lt;xs:enumeration value="bilingual"/&gt;
   &lt;xs:enumeration value="multilingual"/&gt;
 &lt;/xs:restriction&gt;
&lt;/xs:simpleType&gt;
&lt;/xs:element&gt;</pre><p>During the mapping of XML to linked data several decisions have to be taken.
                Sometimes information is being lost. A real example is the mapping of META-SHARE to
                linked data<a href="#ftn.d5e292" class="footnote" name="d5e292"><sup class="footnote">[6]</sup></a>. META-SHARE provides metadata for
                describing linguistic resources.</p><p>Using this element could look like this, expressing that a resource is
                multilingual, e.g. a multilingual dictionary:</p><pre class="programlisting">&lt;lingualityType&gt;multilingual&lt;/lingualityType&gt;</pre><p>A linked data model that represents this data type could look as follows:</p><div class="example"><a name="ex-4"></a><p class="title"><b>Example&nbsp;4.&nbsp;Linked data model that represents parts of the META-SHARE schema</b></p><div class="example-contents"><pre class="programlisting">### Class
ms:linguality
    rdf:type owl:ObjectProperty ;
    rdfs:domain ms:Resource ;
    rdfs:range ms:Linguality .
### Property
ms:Linguality 
    rdf:type owl:Class .
#### Instances
ms:monolingual a ms:Linguality.
ms:bilingual a ms:Linguality.
ms:multilingual a ms:Linguality.</pre></div></div><br class="example-break"><p>This statement expresses the same information like in the XML representation: a
                given resource, e.g. a dictionary, is multilingual. </p><p><span class="bold"><strong>What are the benefits:</strong></span> The benefit of this
                approach is that existing XML workflows don&#8217;t need to be changed at all. The linked
                data representation is just an additional publication channel for the information.
                In this sense, the approach is similar to the RDF chimera discussion. It is however
                still different, since the conversion from XML to RDF involves RDF focused data
                modeling and aims at integration with further linked data sources. </p><p>The additional RDF representation can be integrated with other linked resources
                without influencing the XML workflow. This is what actually is being done with the
                META-SHARE case: via the LingHub portal<a href="#ftn.d5e305" class="footnote" name="d5e305"><sup class="footnote">[7]</sup></a>, META-SHARE
                and other types of metadata for linguistic resources is converted to RDF and being
                made available. A user then can process integrated, multiple linked data sources
                without even knowing that there is an XML representation available. </p><p><span class="bold"><strong>What are the drawbacks:</strong></span> The approach requires a
                completely new tool chain, aiming at linked data integration based on XML (or other
                format based) data sources. The existing setup, e.g. used to analyze the XML
                structures with XQuery or to transform it via XSLT, cannot be used. A query like
                &#8220;Give me all linguistic resources that are multilingual&#8221; can be executed via XPath
                easily in the XML representation. In standard XQuery there is no bridge to SPARQL.
                However, work has been done to create this bridge, see <a class="xref" href="#s5" title="Routes to bridge between RDF and XML">the section called &#8220;Routes to bridge between RDF and XML&#8221;</a>. </p><p>Another drawback of this approach is that it is not always possible to convert XML
                completely into RDF - in particular, RDF has no facility for representing mixed
                content, an essential part of processing textual, human language content. A takeaway
                of <a class="xref" href="#s1-2" title="The Relation to RDF Chimera">the section called &#8220;The Relation to RDF Chimera&#8221;</a> is that data should always be in the format that best fits
                its representation, and URIs can serve as a bridge between formats. The usage of
                URIs for bridging between RDF and XML is discussed in <a class="xref" href="#s3-5" title="Approach 5: Anchor Linked Data via Annotations in XML Content">the section called &#8220;Approach 5: Anchor Linked Data via Annotations in XML Content&#8221;</a>.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s3-2"></a>Approach 2: Embedd Linked Data into XML via Structured Markup</h3></div></div></div><p><span class="bold"><strong>What actually happens: </strong></span>linked data is embedded
                into HTML. Various syntaxes can be used, e.g. JSON-LD, RDFa, or microdata. This
                approach is deployed e.g. in <a class="link" href="http://schema.org/" target="_top">schema.org</a>; see the schema.org homepage for various markup examples.</p><p>We take again the sentence <code class="code">Berlin is the capital of Germany</code> from a
                previous section. The integration of information from Wikipedia with the string
                    <code class="code">Berlin</code> can be achieved as follows: </p><pre class="programlisting">&lt;a itemscope itemtype="http://schema.org/Place" itemprop="url"
 href="https://en.wikipedia.org/wiki/Berlin"&gt;Berlin&lt;/a&gt;</pre><p>With this approach search engines will interpret the
                link <code class="code">https://en.wikipedia.org/wiki/Berlin</code> as a machine readable link to integrate additional information about the place
                Berlin, taken from Wikipedia as a data source. The item being the source of data
                integration is identified as being of type place via the URI
                    <code class="code">http://schema.org/Place</code>. </p><p><span class="bold"><strong>What are the benefits: </strong></span>the approach works well in
                situation in which the hooks for data integration can be embedded into XML or HTML
                content. The search engine optimization scenario via schema.org is the prototypical
                case for doing this. The embedding may also be done in a dedicated markup part for
                metadata using the JSON-LD or other linked data syntaxes, without changing the text
                content; see for details <a class="xref" href="#s3-4" title="Approach 4: Embed Linked Data in Metadata Sections of XML Files">the section called &#8220;Approach 4: Embed Linked Data in Metadata Sections of XML Files&#8221;</a>.</p><p><span class="bold"><strong>What are the drawbacks: </strong></span>RDFa and Microdata
                changes the content and includes new markup. That may not be an option for XML tool
                chains that don&#8217;t &#8220;understand&#8221; the new markup, e.g. lead to validation errors.
                JSON-LD or turtle may have the same issues: where should a tool store this data in
                the XML structure if no general metadata location is available?</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s3-3"></a>Approach 3: Anchor Linked Data in XML Attributes</h3></div></div></div><p><span class="bold"><strong>What actually happens: </strong></span>an identifier is embedded
                in the XML structure. This identifier serves as a bridge between the XML and RDF
                structures. The below example uses the <code class="code">its:taClassRef</code> attribute to
                store the identifier.</p><pre class="programlisting">&lt;source ...&gt;
 &lt;mrk ...<span class="bold"><strong>its:taIdentRef</strong></span>="http://dbpedia.org/resource/Berlin"&gt;
  Berlin&lt;/mrk&gt; is the capital of Germany!&lt;/source&gt;</pre><p>The data integration task, i.e. fetching additional information from linked data
                sources about <code class="code">Berlin</code>, can be executed relying on this information. The
                outcome may then be stored directly in the XML source. Below we assume that the
                population was fetched using the DBpedia information.</p><pre class="programlisting">&lt;source ...&gt;
 &lt;mrk ...its:taIdentRef="http://dbpedia.org/resource/Berlin"&gt;
  Berlin&lt;/mrk&gt; <span class="bold"><strong>(population: 3517424)</strong></span>...&lt;/source&gt;</pre><p>For different purposes, separate linked data queries could be set up. They rely on
                the same identifier <code class="code">http://dbpedia.org/resource/Berlin</code>.</p><p><span class="bold"><strong>What are the benefits: </strong></span>using an XML attribute
                that is already available in the format in question means that no new types of
                markup is needed. That is, existing XML toolchains can stay as is, including
                validation or transformation processes.</p><p><span class="bold"><strong>What are the drawbacks: </strong></span>the data integration is
                postponed. The completed integration, if needed, needs to choose one of the other
                approaches discussed in this paper. Also, the data integration does not leave a
                trace. Further processing steps in the (XML) toolchain cannot identify that the
                string <code class="code">(population: 3517424)</code> is a result of a data integration
                process.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s3-4"></a>Approach 4: Embed Linked Data in Metadata Sections of XML Files</h3></div></div></div><p><span class="bold"><strong>What actually happens: </strong></span>many XML vocabularies have
                metadata sections that may contain arbitrary content. This is also true for XLIFF
                discussed in <a class="xref" href="#s2-2" title="The Case of Vistatec - Linked Data in XML Localization Workflows">the section called &#8220;The Case of Vistatec - Linked Data in XML Localization Workflows&#8221;</a>. The outcome of the linked data processing could
                be stored in such a section.</p><p><span class="bold"><strong>What are the benefits:</strong></span> Compared to <a class="xref" href="#s3-2" title="Approach 2: Embedd Linked Data into XML via Structured Markup">the section called &#8220;Approach 2: Embedd Linked Data into XML via Structured Markup&#8221;</a>, the size of the content itself is not growing with additional, linked data
                related markup. </p><p><span class="bold"><strong>What are the drawbacks:</strong></span> There is no per se
                relation to the content. Like in <a class="xref" href="#ex-2" title="Example&nbsp;2.&nbsp;Linked data representation using offsets for pointing to existing XML content">Example&nbsp;2, &#8220;Linked data representation using offsets for pointing to existing XML content&#8221;</a>, one may create pointers to the XML content, here using character offsets.
                But the pointers may be fragile, if one thinks e.g. of reformatting, insertion or
                deletion of content or markup. In addition, some linked data syntaxes may interfere
                with XML validation or well formedness constraints.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a name="s3-5"></a>Approach 5: Anchor Linked Data via Annotations in XML Content</h3></div></div></div><p><span class="bold"><strong>What actually happens: </strong></span>A generalized approach of
                    <a class="xref" href="#s3-3" title="Approach 3: Anchor Linked Data in XML Attributes">the section called &#8220;Approach 3: Anchor Linked Data in XML Attributes&#8221;</a>
                means that linked data is stored separately from XML structures and that there is a
                reference from linked data to the XML content in question. In <a class="xref" href="#s2-2" title="The Case of Vistatec - Linked Data in XML Localization Workflows">the section called &#8220;The Case of Vistatec - Linked Data in XML Localization Workflows&#8221;</a>, we were using character offsets. The <a class="link" href="http://www.w3.org/TR/2015/WD-annotation-model-20151015/" target="_top">W3C Web
                    Annotation Data Model</a> allows to realize such anchoring. Character offsets
                are just one way of anchoring the annotation. One can also use XPath expressions,
                see the following example.</p><div class="example"><a name="ex-5"></a><p class="title"><b>Example&nbsp;5.&nbsp;Anchoring annotations in XML via the Web annotation data model</b></p><div class="example-contents"><pre class="programlisting">{  "id": "http://example.com/myannotations/a1",
 "type": "Annotation",
 "target": { "type": "SpecificResource",
  "source": "http://example.com/myfile.xml",
  "selector": { "type": "FragmentSelector",
   "conformsTo": "http://www.w3.org/TR/xpath/",
   "value": "/xlf:unit[1]/xlf:segment[1]/xlf:source/xlf:mrk[1]" },
  "itsrdf:taIdentRef": "http://dbpedia.org/resource/Berlin",
  "itsrdf:taClassRef": "http://schema.org/Place",
  "http://dbpedia.org/property/population" : "3517424"  } }</pre></div></div><br class="example-break"><p>The XPath expression in above linked data representation (which uses the JSON-LD
                syntax) selects the XLIFF mrk element from the example in <a class="xref" href="#s3-3" title="Approach 3: Anchor Linked Data in XML Attributes">the section called &#8220;Approach 3: Anchor Linked Data in XML Attributes&#8221;</a>. </p><p><span class="bold"><strong>What are the benefits:</strong></span> In addition to the
                approach 3, here we are able to add the linked data information in the separate
                annotation, e.g. the population of Berlin; there is no need to change the content
                itself. If needed for certain applications, we can use this annotation approach to
                generate others. URIs pointing to the content are an important aspect of such format
                conversions. THe forehand mentioned ITS 2.0 specification shows an example of 1)
                generating <a class="link" href="http://www.w3.org/TR/its20/#conversion-to-nif" target="_top">linked
                    data annotations anchored in XML content</a>, and 2) <a class="link" href="https://www.w3.org/TR/its20/#nif-backconversion" target="_top">integrating the
                        separate annotations into the markup content</a>. The forehand described FREME framework deploys this approach in its ITS enabled <a class="link" href="http://api.freme-project.eu/doc/0.4/knowledge-base/eInternationalization.html" target="_top">e-Internationalisation</a>.</p><p><span class="bold"><strong>What are the drawbacks:</strong></span> the resolution of linked
                data information potentially can be computationally expensive, see e.g. lot&#8217;s of
                XPath expressions to compute for annotations. Also, if the source content changes,
                the anchoring mechanism may not work anymore. Some mechanisms are more robust (e.g.
                XPath expressions), some may be more precise (e.g. the character offset based
                anchoring). </p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s4"></a>Relating Business Cases and Integration Approaches</h2></div></div></div><p>The following table relates the three business cases and the various integration
            approaches.</p><div class="informaltable"><table border="1"><colgroup><col align="left" class="col1"><col align="left" class="col2"><col align="left" class="col3"></colgroup><tbody><tr><td align="left"><span class="bold"><strong>Business case</strong></span></td><td align="left"><span class="bold"><strong>Integration approaches being
                                considered</strong></span></td><td align="left"><span class="bold"><strong>Actual current or experimental
                                praxis</strong></span></td></tr><tr><td align="left"><a class="link" href="#s2-1" title="The Case of Agro-Know and Wolters Kluwer - Linked Data in XML Publishing Workflows">Linked data in XML publishing
                            workflows</a></td><td align="left">Approach 1: convert XML into linked data</td><td align="left">XML workflow kept, linked data conversion scripts to be made
                            available </td></tr><tr><td align="left"><a class="link" href="#s2-2" title="The Case of Vistatec - Linked Data in XML Localization Workflows">Linked data in XML localization
                            workflows</a></td><td align="left">Approach 3: anchor linked data in XML attributes; Approach 4: embed
                            linked data in metadata sections of XML files</td><td align="left">No established practice in localisation industry</td></tr><tr><td align="left"><a class="link" href="#s2-3" title="The Case of iMinds - Linked Data in Book Metadata">Linked data in book metadata</a></td><td align="left">Approach 4: embed linked data in metadata sections of XML
                            files</td><td align="left">No established practice in localisation industry</td></tr></tbody></table></div><p>It becomes obvious that industries take different approaches towards linked data
            integration. This can be explained with availability of native linked data tooling,
            knowledge about its usage, and complexity and potential costs of adapting existing XML
            workflows.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s5"></a>Routes to bridge between RDF and XML</h2></div></div></div><p>As for <span class="bold"><strong>Approach 1</strong></span> (converting XML into linked data),
            in fact existing XML transformation tools like XSLT and XQuery could be used out of the
            box with the caveat that the result is mostly tied to the RDF/XML representation, that
            has various disadvantages, foremost verbosity. More "modern" RDF serializations like
            Turtle or JSON-LD cannot be created out of the box by XML tools straightforwardly, plus
            additional filtering of querying on the resulting RDF triples needs to be encoded
            directly into the XML toolchain, which might be easier solvable in the RDF world itself,
            e.g. using SPARQL. Likewise, as for <span class="bold"><strong>Approach 2</strong></span>, we have
            already identified, that the XML toolchain is not tailored to process and consume RDF or
            similar meta-data formats natively. We face the same problem in <span class="bold"><strong>Approaches 3-5</strong></span>, where RDF-like sources are just linked out of XML
            content, without the necessary toolchain tightly coupled to XML tools that could process
            the RDF content natively.</p><p>So, there is certainly a gap to bridge here in terms of tooling, but recently, that
            partially seems to change: there are academic approaches to encode SPARQL into XML
            processors, such as encoding SPARQL to XSLT or XQuery, cf. e.g. Fischer et al. (2011)
            and Groppe et al. (2008). Plus there are actually some XML processors like SAXON start supporting SPARQL
            natively, cf. <a class="link" href="https://developer.marklogic.com/learn/semantics-exercises/sparql-and-xquery" target="_top">https://developer.marklogic.com/learn/semantics-exercises/sparql-and-xquery</a>.</p><p>Alternatively, given that SPARQL actually can produce XML or JSON (among others) as
            output format<a href="#ftn.d5e420" class="footnote" name="d5e420"><sup class="footnote">[8]</sup></a>, it is possible to directly consume the
            results of SPARQL queries in XML tools, however more complex use cases need some
            scripting around this, plus intermediate results for an overall transformation need to
            be stored and processed separately, ending up in a heterogeneous toolchain, comprising
            XML tools, SPARQL processors and potentially even another scripting language on top. </p><p>Additionally, there are new &#8220;hybrid&#8221; but integrated toolchains arising that try to
            combine the two worlds of XML and RDF in a &#8220;best-of-both-worlds&#8221; approach: most
            prominently, we&#8217;d like to mention as an example here the XSPARQL project<a href="#ftn.d5e424" class="footnote" name="d5e424"><sup class="footnote">[9]</sup></a>, that aims at integrating SPARQL into XQuery in a compilation
            approach: that is, queries on RDF data or to a remote SPARQL endpoint serving Linked
            Data can be embedded into an XQuery. For instance, the following query transforms
            geographic RDF data queried from the file <a class="link" href="http://nunolopes.org/foaf.rdf" target="_top">http://nunolopes.org/foaf.rdf</a> into
            a KML file: </p><pre class="programlisting">prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt;
prefix geo: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; 

&lt;kml xmlns="http://www.opengis.net/kml/2.2"&gt;{
 <span class="bold"><strong>for $name $long $lat from &lt;http://nunolopes.org/foaf.rdf&gt;
 where { $person a foaf:Person; foaf:name $name;
         foaf:based_near [ a geo:Point;
         geo:long $long; 
         geo:lat $lat ] }</strong></span> 
 return &lt;Placemark&gt;
  &lt;name&gt;{fn:concat("Location of ", $name)}&lt;/name&gt;
   &lt;Point&gt;
    &lt;coordinates&gt;{fn:concat($long, ",", $lat, ",0")}&lt;/coordinates&gt;
   &lt;/Point&gt;
 &lt;/Placemark&gt; }
&lt;/kml&gt;</pre><p>The boldface part of the above query is actually SPARQL syntax embedded into XQuery.
            An XSPARQL compiler translates this query to a native XQuery that delegates these query
            parts to a native SPARQL query processor. More details on this approach can be found in
            Bischof et al. (2012).</p><p>Note that also the other way, i.e. transforming XML data into RDF is supported in this
            approach, by allowing SPARQL&#8217;s CONSTRUCT clauses in the return claus of an XQuery, as
            shown in the following short example, which transforms XML data from Open Streetmap to
            RDF:</p><pre class="programlisting">prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt;
prefix geo: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt;
prefix kml: &lt;http://earth.google.com/kml/2.0&gt;
let $loc := "WU Wien" for $place in doc
  (fn:concat("http://nominatim.openstreetmap.org/search?q=",
    fn:encode-for-uri($loc),
    "&amp;amp;format=xml"))
let $geo := fn:tokenize($place//place[1]/@boundingbox, ",")
<span class="bold"><strong>construct { &lt;http://www.polleres.net/foaf.rdf#me&gt;
foaf:based_near [ geo:lat {$geo[1]}; geo:long {$geo[3]} ] }</strong></span></pre><p>More recently, XSPARQL has been extended to cover the recent SPARQL1.1 specification,
            plus support for JSON as input and output format (by internally representing JSON in a
            canonical XML representation have been added, cf. Dell&#8217;Aglio et al. (2014).</p><p>While XSPARQL, which has actually started as a W3C member submission in 2009<a href="#ftn.d5e436" class="footnote" name="d5e436"><sup class="footnote">[10]</sup></a>, is just one possible route, the authors believe that joint
            efforts in standardization bodies to bridge the gaps between RDF and XML in order to
            enable such transformations and integrated tooling in a standard way should be further
            pursued.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s-6"></a>Conclusion</h2></div></div></div><p>This paper discussed the motivation for integrating RDF and XML. We looked at various
            business case scenarios that can benefit from this integration. We then discussed
            several approaches to realize the integration. Finally, we looked into technical
            solutions that integrate the actual XML and RDF technology stacks.</p><p>A reviewer of this paper suggested consider XProc for integrating RDF and XML
            workflows. <a class="link" href="https://www.w3.org/TR/xproc20/" target="_top">XProc 2.0</a> will have the
            ability to pass information other than XML data between steps; it would be possible to
            pass RDF data between XProc steps and have filtering and processing steps for that RDF
            data. This would allow processing of XML data with XML tools (XSLT, XQuery), while
            tracking and also processing RDF data with e.g. SPARQL or XSPARQL. This approach sounds
            promising but has not been explored in this paper, so we leave it to future work.</p><p>A next steps could be to discuss the integration approaches in a broader community,
            e.g. in a dedicated forum like a W3C community group. This could also help to move the
            forehand described XML - RDF standardization work forward. Such standardization has been
            discussed in the past. It is the hope of the authors of this paper that it brings new
            insights to this discussion, with the real-life needs from actual applications who more
            and more are in need of the integration.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="s-ack"></a>Acknowledgments</h2></div></div></div><p>The creation of this paper was supported by the FREME project under the Horizon 2020 Framework Programme of the European Commission, Grant Agreement Number 644771.</p></div><div class="bibliography"><div class="titlepage"><div><div><h2 class="title"><a name="sec-references"></a>Bibliography</h2></div></div></div><div class="bibliomixed"><a name="d5e449"></a><p class="bibliomixed">Bischof, S., S. Decker, T. Krennwallner, N. Lopes and A. Polleres. <span class="title">Mapping between RDF and XML with XSPARQL.</span> Journal on Data Semantics (JoDS), 1(3):147-185, 2012.</p></div><div class="bibliomixed"><a name="d5e451"></a><p class="bibliomixed">Dell'Aglio, D., A. Polleres, N. Lopes and S. Bischof. <span class="title">Querying the web of data with XSPARQL 1.1.</span> In ISWC2014 Developers Workshop, volume 1268 of CEUR Workshop Proceedings. CEUR-WS.org, October 2014.</p></div><div class="bibliomixed"><a name="d5e453"></a><p class="bibliomixed">Fischer, P., D. Florescu, M. Kaufmann and D. Kossmann D (2011). <span class="title">Translating SPARQL and SQL to XQuery.</span> In: Proceedings of XML Prague&#8217;11, pp 81&#8211;98.</p></div><div class="bibliomixed"><a name="d5e455"></a><p class="bibliomixed">Groppe S., J. Groppe, V. Linnemann, D. Kukulenz, N. Hoeller, C. Reinke (2008). <span class="title">Embedding SPARQL into XQuery/XSLT.</span> In: SAC&#8217;08.  ACM, New York, pp 2271&#8211;2278.</p></div></div><div class="footnotes"><br><hr style="width:100; text-align:left;margin-left: 0"><div id="ftn.d5e114" class="footnote"><p><a href="#d5e114" class="para"><sup class="para">[1] </sup></a>See <a class="link" href="http://www.w3.org/2014/data-shapes/" target="_top">http://www.w3.org/2014/data-shapes/</a>.</p></div><div id="ftn.d5e131" class="footnote"><p><a href="#d5e131" class="para"><sup class="para">[2] </sup></a>See the FREME project homepage at <a class="link" href="http://freme-project.eu/" target="_top">http://freme-project.eu/</a> for more
                information.</p></div><div id="ftn.d5e226" class="footnote"><p><a href="#d5e226" class="para"><sup class="para">[3] </sup></a>See <a class="link" href="http://api.freme-project.eu/doc/0.4/tutorials/spot-entities-in-xliff.html" target="_top">http://api.freme-project.eu/doc/0.4/tutorials/spot-entities-in-xliff.html</a></p></div><div id="ftn.d5e254" class="footnote"><p><a href="#d5e254" class="para"><sup class="para">[4] </sup></a>See <a class="link" href="http://www.editeur.org/83/Overview/" target="_top">http://www.editeur.org/83/Overview/</a></p></div><div id="ftn.d5e258" class="footnote"><p><a href="#d5e258" class="para"><sup class="para">[5] </sup></a>See <a class="link" href="http://www.editeur.org/" target="_top">http://www.editeur.org/</a>.</p></div><div id="ftn.d5e292" class="footnote"><p><a href="#d5e292" class="para"><sup class="para">[6] </sup></a>See <a class="link" href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/664_Paper.pdf" target="_top">http://www.lrec-conf.org/proceedings/lrec2014/pdf/664_Paper.pdf</a> for more
                    details on the META-SHARE case and on how the conversion from XML to linked data was
                    achieved.</p></div><div id="ftn.d5e305" class="footnote"><p><a href="#d5e305" class="para"><sup class="para">[7] </sup></a>See <a class="link" href="http://linghub.lider-project.eu/" target="_top">http://linghub.lider-project.eu/</a>.</p></div><div id="ftn.d5e420" class="footnote"><p><a href="#d5e420" class="para"><sup class="para">[8] </sup></a>See <a class="link" href="http://www.w3.org/TR/sparql11-overview/#sparql11-results" target="_top">http://www.w3.org/TR/sparql11-overview/#sparql11-results</a> for details.</p></div><div id="ftn.d5e424" class="footnote"><p><a href="#d5e424" class="para"><sup class="para">[9] </sup></a>See <a class="link" href="http://xsparql.sourceforge.net" target="_top">http://xsparql.sourceforge.net</a> for details.</p></div><div id="ftn.d5e436" class="footnote"><p><a href="#d5e436" class="para"><sup class="para">[10] </sup></a>See <a class="link" href="http://www.w3.org/Submission/2009/01/" target="_top">http://www.w3.org/Submission/2009/01/</a>.</p></div></div></div></body></html>